{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1940,
          "sourceType": "datasetVersion",
          "datasetId": 1069
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T05:38:19.501828Z",
          "iopub.execute_input": "2025-07-26T05:38:19.502469Z",
          "iopub.status.idle": "2025-07-26T05:38:20.321828Z",
          "shell.execute_reply.started": "2025-07-26T05:38:19.502445Z",
          "shell.execute_reply": "2025-07-26T05:38:20.321175Z"
        },
        "id": "KWfGhf0QarPX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 1: Imports and Setup ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T05:38:26.447913Z",
          "iopub.execute_input": "2025-07-26T05:38:26.448823Z",
          "iopub.status.idle": "2025-07-26T05:38:27.539015Z",
          "shell.execute_reply.started": "2025-07-26T05:38:26.448794Z",
          "shell.execute_reply": "2025-07-26T05:38:27.538156Z"
        },
        "id": "WF-gKSWcarPa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 2: Data Loading and Preprocessing ---\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/PS_20174392719_1491204439457_log.csv\")\n",
        "\n",
        "# Clean and preprocess data\n",
        "df = df.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
        "df = pd.get_dummies(df, columns=['type'], prefix='type')\n",
        "df = df.replace({False: 0, True: 1})\n",
        "\n",
        "# Scale numerical columns\n",
        "columns_to_normalize = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
        "scaler = MinMaxScaler()\n",
        "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "y = df['isFraud']\n",
        "X = df.drop(['isFraud'], axis=1)\n",
        "\n",
        "# Split data: train VAE on normal data only\n",
        "X_train = X[y == 0]\n",
        "X_test = X.copy()\n",
        "y_test = y.copy()\n",
        "\n",
        "# Create Tensors\n",
        "train_tensor = torch.FloatTensor(X_train.values)\n",
        "test_tensor = torch.FloatTensor(X_test.values)\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T05:38:31.686595Z",
          "iopub.execute_input": "2025-07-26T05:38:31.687513Z",
          "iopub.status.idle": "2025-07-26T05:39:01.112106Z",
          "shell.execute_reply.started": "2025-07-26T05:38:31.687484Z",
          "shell.execute_reply": "2025-07-26T05:39:01.111157Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpZjodVtarPb",
        "outputId": "6aaacad6-5d04-4fb7-9f79-1ae468a85ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-19916438.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.replace({False: 0, True: 1})\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: VAE Model Definition ---\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=8):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 16), nn.ReLU()\n",
        "        )\n",
        "        self.mu = nn.Linear(16, latent_dim)\n",
        "        self.logvar = nn.Linear(16, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 16), nn.ReLU(),\n",
        "            nn.Linear(16, 32), nn.ReLU(),\n",
        "            nn.Linear(32, input_dim), nn.Sigmoid() # Sigmoid is good since data is scaled 0-1\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "def loss_fn(x, x_hat, mu, logvar):\n",
        "    recon_loss = nn.functional.mse_loss(x_hat, x, reduction='sum') # use sum for better scaling\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_loss"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T05:39:11.843747Z",
          "iopub.execute_input": "2025-07-26T05:39:11.844792Z",
          "iopub.status.idle": "2025-07-26T05:39:11.853798Z",
          "shell.execute_reply.started": "2025-07-26T05:39:11.844736Z",
          "shell.execute_reply": "2025-07-26T05:39:11.852943Z"
        },
        "id": "DRlMlPtsarPc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: Baseline VAE Training ---\n",
        "print(\"--- Training Baseline VAE ---\")\n",
        "vae = VAE(input_dim=X.shape[1])\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(5): # Increased epochs for better training\n",
        "    vae.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        x = batch[0]\n",
        "        x_hat, mu, logvar = vae(x)\n",
        "        loss = loss_fn(x, x_hat, mu, logvar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T06:03:01.439087Z",
          "iopub.execute_input": "2025-07-26T06:03:01.439357Z",
          "iopub.status.idle": "2025-07-26T06:15:17.040144Z",
          "shell.execute_reply.started": "2025-07-26T06:03:01.439338Z",
          "shell.execute_reply": "2025-07-26T06:15:17.039398Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLwyL2acarPc",
        "outputId": "21c59d1a-be50-4d9d-98d7-dd10d3d035ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Baseline VAE ---\n",
            "Epoch 1: Loss = 0.7433\n",
            "Epoch 2: Loss = 0.7247\n",
            "Epoch 3: Loss = 0.7246\n",
            "Epoch 4: Loss = 0.7245\n",
            "Epoch 5: Loss = 0.7245\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 5: Baseline VAE Evaluation ---\n",
        "print(\"\\n--- Evaluating Baseline VAE ---\")\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    x_hat, _, _ = vae(test_tensor)\n",
        "    recon_error = torch.mean((x_hat - test_tensor) ** 2, dim=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T06:16:09.522541Z",
          "iopub.execute_input": "2025-07-26T06:16:09.523308Z",
          "iopub.status.idle": "2025-07-26T06:16:14.201269Z",
          "shell.execute_reply.started": "2025-07-26T06:16:09.523281Z",
          "shell.execute_reply": "2025-07-26T06:16:14.200424Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1WGo1nsarPd",
        "outputId": "33fc46f0-a479-4354-c628-aa4257e192c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Baseline VAE ---\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create results DataFrame with proper data type handling\n",
        "results = pd.DataFrame({\n",
        "    'recon_error': recon_error.detach().cpu().numpy().astype(np.float64),\n",
        "    'true_class': y_test.values if hasattr(y_test, 'values') else y_test\n",
        "})\n",
        "\n",
        "# Check for and handle problematic values\n",
        "print(f\"Results shape: {results.shape}\")\n",
        "print(f\"NaN values in recon_error: {results['recon_error'].isna().sum()}\")\n",
        "print(f\"Infinite values in recon_error: {np.isinf(results['recon_error']).sum()}\")\n",
        "\n",
        "# Remove any rows with NaN or infinite reconstruction errors\n",
        "results = results.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "print(f\"Clean results shape after removing NaN/inf: {results.shape}\")\n",
        "\n",
        "# Plot the distribution of reconstruction errors for both classes\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=results[results['true_class'] == 0], x='recon_error', color=\"blue\", label=\"Normal\", bins=50, stat=\"density\", common_norm=False)\n",
        "sns.histplot(data=results[results['true_class'] == 1], x='recon_error', color=\"red\", label=\"Fraud\", bins=50, stat=\"density\", common_norm=False)\n",
        "plt.title('Distribution of Reconstruction Errors (Baseline VAE)')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T06:16:52.271150Z",
          "iopub.execute_input": "2025-07-26T06:16:52.271948Z",
          "iopub.status.idle": "2025-07-26T06:16:54.425905Z",
          "shell.execute_reply.started": "2025-07-26T06:16:52.271921Z",
          "shell.execute_reply": "2025-07-26T06:16:54.425045Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "jGuBam7FarPd",
        "outputId": "9dae06ee-5094-49d3-c999-489106c3a467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results shape: (886340, 2)\n",
            "NaN values in recon_error: 1\n",
            "Infinite values in recon_error: 0\n",
            "Clean results shape after removing NaN/inf: (886339, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add positional encoding to help transformer understand transaction order\"\"\"\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T06:18:42.001907Z",
          "iopub.execute_input": "2025-07-26T06:18:42.002458Z",
          "iopub.status.idle": "2025-07-26T06:18:42.009138Z",
          "shell.execute_reply.started": "2025-07-26T06:18:42.002430Z",
          "shell.execute_reply": "2025-07-26T06:18:42.008259Z"
        },
        "id": "G7f0KALJarPe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set threshold using cleaned data\n",
        "threshold = results[results['true_class'] == 0]['recon_error'].quantile(0.99)\n",
        "print(f\"Threshold for fraud detection: {threshold:.6f}\")\n",
        "\n",
        "# Make predictions with proper data types\n",
        "y_pred = (results['recon_error'] > threshold).astype(int)\n",
        "y_true = results['true_class'].astype(int)\n",
        "\n",
        "# Ensure both arrays have the same length and proper data types\n",
        "print(f\"Predictions shape: {y_pred.shape}, True labels shape: {y_true.shape}\")\n",
        "\n",
        "# Evaluate Performance\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report (Baseline VAE):\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Normal', 'Fraud']))\n",
        "\n",
        "print(f\"\\nROC AUC Score: {roc_auc_score(y_true, results['recon_error']):.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-26T06:16:58.642757Z",
          "iopub.execute_input": "2025-07-26T06:16:58.643265Z",
          "iopub.status.idle": "2025-07-26T06:17:36.881189Z",
          "shell.execute_reply.started": "2025-07-26T06:16:58.643241Z",
          "shell.execute_reply": "2025-07-26T06:17:36.880464Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBp9dWLrarPe",
        "outputId": "d39ca6ff-727c-4373-c554-0d25016e6e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold for fraud detection: 0.119010\n",
            "Predictions shape: (886339,), True labels shape: (886339,)\n",
            "\n",
            "Confusion Matrix:\n",
            "[[876986   8859]\n",
            " [   474     20]]\n",
            "\n",
            "Classification Report (Baseline VAE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.99      0.99    885845\n",
            "       Fraud       0.00      0.04      0.00       494\n",
            "\n",
            "    accuracy                           0.99    886339\n",
            "   macro avg       0.50      0.52      0.50    886339\n",
            "weighted avg       1.00      0.99      0.99    886339\n",
            "\n",
            "\n",
            "ROC AUC Score: 0.5989\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# SAVE BASELINE VAE TO GOOGLE DRIVE\n",
        "# ================================\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Create models directory in Google Drive\n",
        "models_dir = '/content/drive/MyDrive/fraud_detection_models'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "print(f\" Models directory created: {models_dir}\")\n",
        "\n",
        "# Step 3: Save the trained baseline VAE\n",
        "print(\"Saving baseline VAE model...\")\n",
        "\n",
        "# Calculate ROC AUC for filename (assuming you have results from evaluation)\n",
        "baseline_roc_auc = roc_auc_score(y_true, results[\"recon_error\"])\n",
        "\n",
        "# Create filename with performance metric\n",
        "model_filename = f'{models_dir}/baseline_vae_roc_{baseline_roc_auc:.4f}_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.pth'\n",
        "\n",
        "# Save comprehensive checkpoint\n",
        "torch.save({\n",
        "    'model_state_dict': vae.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'model_config': {\n",
        "        'input_dim': X.shape[1],\n",
        "        'latent_dim': 8  # Adjust based on your VAE latent dimension\n",
        "    },\n",
        "    'training_info': {\n",
        "        'epochs': 5,  # Adjust based on your training\n",
        "        'learning_rate': 1e-3,\n",
        "        'batch_size': 128,\n",
        "        'roc_auc': baseline_roc_auc,\n",
        "        'threshold': threshold,\n",
        "        'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'dataset_size': len(X)\n",
        "    },\n",
        "    'scaler': scaler,  # Your StandardScaler\n",
        "    'results_baseline': results,  # Your evaluation results\n",
        "    'feature_names': list(X.columns) if hasattr(X, 'columns') else None\n",
        "}, model_filename)\n",
        "\n",
        "print(f\" Baseline VAE model saved successfully!\")\n",
        "print(f\" Location: {model_filename}\")\n",
        "print(f\" Performance: ROC AUC = {baseline_roc_auc:.4f}\")\n",
        "print(f\" File size: {os.path.getsize(model_filename) / (1024*1024):.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djZZlDJqtGY6",
        "outputId": "37036137-c541-451a-844d-a62342e5d4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            " Models directory created: /content/drive/MyDrive/fraud_detection_models\n",
            "Saving baseline VAE model...\n",
            " Baseline VAE model saved successfully!\n",
            " Location: /content/drive/MyDrive/fraud_detection_models/baseline_vae_roc_0.5989_20250726_1229.pth\n",
            " Performance: ROC AUC = 0.5989\n",
            " File size: 23.15 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "import math\n",
        "\n",
        "# ================================\n",
        "# 1. INDIVIDUAL TRANSACTION TRANSFORMER\n",
        "# ================================\n",
        "\n",
        "class TransactionTransformer(nn.Module):\n",
        "    \"\"\"Transformer for individual transaction feature enhancement\"\"\"\n",
        "    def __init__(self, feature_dim, d_model=128, nhead=8, num_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature embedding\n",
        "        self.feature_embedding = nn.Linear(feature_dim, d_model)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Self-attention to learn feature relationships\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=0.1,\n",
        "            activation='relu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, feature_dim]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Embed features\n",
        "        embedded = self.feature_embedding(x)  # [batch_size, d_model]\n",
        "        embedded = self.layer_norm1(embedded)\n",
        "\n",
        "        # Add sequence dimension for transformer (treat each feature as a \"token\")\n",
        "        # Reshape to treat features as sequence elements\n",
        "        embedded = embedded.unsqueeze(1)  # [batch_size, 1, d_model]\n",
        "\n",
        "        # Apply transformer\n",
        "        transformed = self.transformer(embedded)  # [batch_size, 1, d_model]\n",
        "\n",
        "        # Remove sequence dimension and project\n",
        "        output = transformed.squeeze(1)  # [batch_size, d_model]\n",
        "        output = self.output_projection(output)\n",
        "        output = self.layer_norm2(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "cfktj5wDt_6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 2. VAE (Reuse your existing VAE class)\n",
        "# ================================\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.mu_layer = nn.Linear(64, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(64, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar"
      ],
      "metadata": {
        "id": "V4n2KWM8uFhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================\n",
        "# 3. LOSS FUNCTION\n",
        "# ================================\n",
        "\n",
        "def loss_fn(x, x_hat, mu, logvar):\n",
        "    # Reconstruction loss\n",
        "    recon_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return recon_loss + 0.01 * kl_loss  # Weight KL loss"
      ],
      "metadata": {
        "id": "9MI7UNtquJxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================\n",
        "# 4. MAIN IMPLEMENTATION\n",
        "# ================================\n",
        "\n",
        "print(\"\\n=== Individual Transaction Transformer + VAE Implementation ===\")\n",
        "\n",
        "# Step 1: Prepare data (using your existing X, y)\n",
        "print(\"Step 1: Preparing transaction data...\")\n",
        "\n",
        "# Scale features\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "X_tensor = torch.FloatTensor(X_scaled)\n",
        "y_tensor = torch.LongTensor(y.values)\n",
        "\n",
        "print(f\"Dataset shape: {X_tensor.shape}\")\n",
        "print(f\"Class distribution: Normal={sum(y==0)}, Fraud={sum(y==1)}\")\n",
        "\n",
        "# Step 2: Create and train transaction transformer\n",
        "print(\"\\nStep 2: Training Transaction Transformer...\")\n",
        "\n",
        "# Initialize transformer\n",
        "transaction_transformer = TransactionTransformer(\n",
        "    feature_dim=X_tensor.shape[1],\n",
        "    d_model=128,\n",
        "    nhead=8,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "# For this example, we'll use the transformer in inference mode\n",
        "# In practice, you could pre-train it on a related task or use it as-is\n",
        "transaction_transformer.eval()\n",
        "\n",
        "# Generate enhanced embeddings for all transactions\n",
        "with torch.no_grad():\n",
        "    enhanced_embeddings = transaction_transformer(X_tensor)\n",
        "\n",
        "print(f\"Enhanced embeddings shape: {enhanced_embeddings.shape}\")\n",
        "\n",
        "# Step 3: Train VAE on normal transactions only\n",
        "print(\"\\nStep 3: Training VAE on normal transactions...\")\n",
        "\n",
        "# Filter normal transactions\n",
        "normal_mask = (y_tensor == 0)\n",
        "normal_embeddings = enhanced_embeddings[normal_mask]\n",
        "\n",
        "print(f\"Normal transactions for VAE training: {len(normal_embeddings)}\")\n",
        "\n",
        "# Initialize VAE\n",
        "embedding_vae = VAE(input_dim=128, latent_dim=16)\n",
        "optimizer = optim.Adam(embedding_vae.parameters(), lr=1e-3)\n",
        "\n",
        "# Create data loader for normal embeddings\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(normal_embeddings),\n",
        "    batch_size=256,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KtaDacXuNrr",
        "outputId": "c0df40e2-c1c8-41ea-c041-121b64890e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Individual Transaction Transformer + VAE Implementation ===\n",
            "Step 1: Preparing transaction data...\n",
            "Dataset shape: torch.Size([886340, 11])\n",
            "Class distribution: Normal=885845, Fraud=494\n",
            "\n",
            "Step 2: Training Transaction Transformer...\n",
            "Enhanced embeddings shape: torch.Size([886340, 128])\n",
            "\n",
            "Step 3: Training VAE on normal transactions...\n",
            "Normal transactions for VAE training: 885845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train VAE\n",
        "embedding_vae.train()\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        x_batch = batch[0]\n",
        "\n",
        "        # Forward pass\n",
        "        x_recon, mu, logvar = embedding_vae(x_batch)\n",
        "        loss = loss_fn(x_batch, x_recon, mu, logvar)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(normal_embeddings)\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Average Loss = {avg_loss:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n6uSH21uSFx",
        "outputId": "5db27833-7eb5-4957-d4a2-4583ed24122b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Average Loss = 0.156716\n",
            "Epoch 4: Average Loss = 0.150910\n",
            "Epoch 6: Average Loss = 0.149894\n",
            "Epoch 8: Average Loss = 0.153680\n",
            "Epoch 10: Average Loss = 0.152683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Evaluate on all transactions\n",
        "print(\"\\nStep 4: Evaluating fraud detection performance...\")\n",
        "\n",
        "embedding_vae.eval()\n",
        "with torch.no_grad():\n",
        "    # Get reconstruction errors for all transactions\n",
        "    all_recon, _, _ = embedding_vae(enhanced_embeddings)\n",
        "    reconstruction_errors = torch.mean((all_recon - enhanced_embeddings) ** 2, dim=1)\n",
        "\n",
        "# Convert to numpy for analysis\n",
        "recon_errors_np = reconstruction_errors.numpy()\n",
        "y_true = y_tensor.numpy()\n",
        "\n",
        "# ===== HANDLE NaN VALUES =====\n",
        "# Check for NaN values and handle them\n",
        "nan_mask = np.isnan(recon_errors_np)\n",
        "nan_count = nan_mask.sum()\n",
        "\n",
        "if nan_count > 0:\n",
        "    print(f\"  Found {nan_count} NaN values in reconstruction errors\")\n",
        "    print(\"Replacing NaN values with maximum finite reconstruction error...\")\n",
        "\n",
        "    # Replace NaN with a high error value (assuming higher error = more likely fraud)\n",
        "    finite_errors = recon_errors_np[~nan_mask]\n",
        "    if len(finite_errors) > 0:\n",
        "        max_finite_error = np.max(finite_errors)\n",
        "        recon_errors_np[nan_mask] = max_finite_error * 2  # Set to high value\n",
        "    else:\n",
        "        print(\" All reconstruction errors are NaN! Model training failed.\")\n",
        "        # Set to arbitrary high values\n",
        "        recon_errors_np[nan_mask] = 1.0\n",
        "\n",
        "    print(f\" NaN values replaced. New range: [{np.min(recon_errors_np):.6f}, {np.max(recon_errors_np):.6f}]\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_individual = pd.DataFrame({\n",
        "    'recon_error': recon_errors_np,\n",
        "    'true_class': y_true\n",
        "})\n",
        "\n",
        "\n",
        "# Set threshold (99th percentile of normal reconstruction errors)\n",
        "normal_errors = results_individual[results_individual['true_class'] == 0]['recon_error']\n",
        "threshold_individual = normal_errors.quantile(0.99)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_individual = (results_individual['recon_error'] > threshold_individual).astype(int)\n",
        "\n",
        "print(f\"Threshold: {threshold_individual:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A2NWYcDv8A4",
        "outputId": "39d4b24a-941c-4113-dd70-428e96672b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Evaluating fraud detection performance...\n",
            "  Found 1 NaN values in reconstruction errors\n",
            "Replacing NaN values with maximum finite reconstruction error...\n",
            " NaN values replaced. New range: [0.000002, 2.004113]\n",
            "Threshold: 0.001515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Calculate metrics\n",
        "print(\"\\n=== RESULTS ===\")\n",
        "\n",
        "# ===== FIX: FILTER TO BINARY CLASSES ONLY =====\n",
        "print(\" Checking class distribution...\")\n",
        "unique_classes = np.unique(y_true)\n",
        "print(f\"Classes found: {unique_classes}\")\n",
        "\n",
        "# Filter to only binary classes (0 and 1)\n",
        "binary_mask = np.isin(y_true, [0, 1])\n",
        "y_true_binary = y_true[binary_mask]\n",
        "recon_errors_binary = recon_errors_np[binary_mask]\n",
        "y_pred_binary = y_pred_individual[binary_mask]\n",
        "\n",
        "print(f\"Original dataset size: {len(y_true)}\")\n",
        "print(f\"Filtered binary dataset size: {len(y_true_binary)}\")\n",
        "\n",
        "# Check binary class distribution\n",
        "unique, counts = np.unique(y_true_binary, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    class_name = \"Normal\" if cls == 0 else \"Fraud\"\n",
        "    print(f\"  {class_name} (class {cls}): {count} transactions\")\n",
        "\n",
        "# ===== END FIX =====\n",
        "\n",
        "# ROC AUC (now with binary data)\n",
        "individual_auc = roc_auc_score(y_true_binary, recon_errors_binary)\n",
        "print(f\"\\n ROC AUC Score: {individual_auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_binary, y_pred_binary, target_names=['Normal', 'Fraud']))\n",
        "\n",
        "# Additional metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"\\n Detailed Metrics:\")\n",
        "print(f\"   - ROC AUC: {individual_auc:.4f}\")\n",
        "print(f\"   - Precision: {precision:.4f}\")\n",
        "print(f\"   - Recall: {recall:.4f}\")\n",
        "print(f\"   - F1-Score: {f1:.4f}\")\n",
        "print(f\"   - True Positives: {tp}\")\n",
        "print(f\"   - False Positives: {fp}\")\n",
        "print(f\"   - True Negatives: {tn}\")\n",
        "print(f\"   - False Negatives: {fn}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPR3g5SswAW4",
        "outputId": "ddcaad23-ea35-4ed8-bdbd-8bd199dcbbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== RESULTS ===\n",
            " Checking class distribution...\n",
            "Classes found: [-9223372036854775808                    0                    1]\n",
            "Original dataset size: 886340\n",
            "Filtered binary dataset size: 886339\n",
            "  Normal (class 0): 885845 transactions\n",
            "  Fraud (class 1): 494 transactions\n",
            "\n",
            " ROC AUC Score: 0.7680\n",
            "\n",
            "Confusion Matrix:\n",
            "[[876986   8859]\n",
            " [   358    136]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.99      0.99    885845\n",
            "       Fraud       0.02      0.28      0.03       494\n",
            "\n",
            "    accuracy                           0.99    886339\n",
            "   macro avg       0.51      0.63      0.51    886339\n",
            "weighted avg       1.00      0.99      0.99    886339\n",
            "\n",
            "\n",
            " Detailed Metrics:\n",
            "   - ROC AUC: 0.7680\n",
            "   - Precision: 0.0151\n",
            "   - Recall: 0.2753\n",
            "   - F1-Score: 0.0287\n",
            "   - True Positives: 136\n",
            "   - False Positives: 8859\n",
            "   - True Negatives: 876986\n",
            "   - False Negatives: 358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Analyze reconstruction error distributions\n",
        "print(\"\\n Reconstruction Error Analysis:\")\n",
        "normal_errors_stats = normal_errors.describe()\n",
        "fraud_errors = results_individual[results_individual['true_class'] == 1]['recon_error']\n",
        "fraud_errors_stats = fraud_errors.describe()\n",
        "\n",
        "print(\"\\nNormal Transaction Errors:\")\n",
        "print(f\"  Mean: {normal_errors_stats['mean']:.6f}\")\n",
        "print(f\"  Std:  {normal_errors_stats['std']:.6f}\")\n",
        "print(f\"  99th percentile: {normal_errors.quantile(0.99):.6f}\")\n",
        "\n",
        "print(\"\\nFraud Transaction Errors:\")\n",
        "print(f\"  Mean: {fraud_errors_stats['mean']:.6f}\")\n",
        "print(f\"  Std:  {fraud_errors_stats['std']:.6f}\")\n",
        "print(f\"  Min:  {fraud_errors_stats['min']:.6f}\")\n",
        "print(f\"  Max:  {fraud_errors_stats['max']:.6f}\")\n",
        "\n",
        "print(\"\\n Individual Transaction Processing Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6WvYd-NuUuy",
        "outputId": "9b357d98-01bc-4478-c930-6b2b08cc4bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Reconstruction Error Analysis:\n",
            "\n",
            "Normal Transaction Errors:\n",
            "  Mean: 0.000326\n",
            "  Std:  0.001321\n",
            "  99th percentile: 0.001515\n",
            "\n",
            "Fraud Transaction Errors:\n",
            "  Mean: 0.004531\n",
            "  Std:  0.010911\n",
            "  Min:  0.000058\n",
            "  Max:  0.123834\n",
            "\n",
            " Individual Transaction Processing Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# SAVE ENHANCED TRANSFORMER + VAE TO GOOGLE DRIVE\n",
        "# ================================\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"\\n Saving Enhanced Transformer + VAE Model...\")\n",
        "\n",
        "# Ensure Google Drive is mounted\n",
        "try:\n",
        "    if not os.path.exists('/content/drive/MyDrive'):\n",
        "        drive.mount('/content/drive')\n",
        "except:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Create models directory\n",
        "models_dir = '/content/drive/MyDrive/fraud_detection_models'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# Calculate performance metrics for filename\n",
        "enhanced_roc_auc = individual_auc  # Your ROC AUC: 0.7921\n",
        "\n",
        "# Create filename with performance and architecture info\n",
        "model_filename = f'{models_dir}/enhanced_transformer_vae_roc_{enhanced_roc_auc:.4f}_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.pth'\n",
        "\n",
        "# Save comprehensive checkpoint\n",
        "torch.save({\n",
        "    # Model components\n",
        "    'transformer_state_dict': transaction_transformer.state_dict(),\n",
        "    'vae_state_dict': embedding_vae.state_dict(),\n",
        "    'vae_optimizer_state_dict': optimizer.state_dict(),\n",
        "\n",
        "    # Model architecture config\n",
        "    'model_config': {\n",
        "        'transformer_config': {\n",
        "            'feature_dim': X_tensor.shape[1],\n",
        "            'd_model': 128,\n",
        "            'nhead': 8,\n",
        "            'num_layers': 2\n",
        "        },\n",
        "        'vae_config': {\n",
        "            'input_dim': 128,  # transformer output dim\n",
        "            'latent_dim': 16\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # Training information\n",
        "    'training_info': {\n",
        "        'epochs': 10,\n",
        "        'learning_rate': 1e-3,\n",
        "        'batch_size': 256,\n",
        "        'threshold_percentile': 99,  # 99th percentile\n",
        "        'roc_auc': enhanced_roc_auc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'threshold_value': threshold_individual,\n",
        "        'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'dataset_size': len(X_tensor),\n",
        "        'architecture': 'Individual_Transaction_Transformer_VAE'\n",
        "    },\n",
        "\n",
        "    # Data preprocessing\n",
        "    'scaler': scaler_X,  # Your StandardScaler\n",
        "\n",
        "    # Results and metrics\n",
        "    'results_enhanced': results_individual,\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'feature_names': list(X.columns) if hasattr(X, 'columns') else None,\n",
        "\n",
        "    # Additional analysis\n",
        "    'reconstruction_stats': {\n",
        "        'normal_error_mean': normal_errors.mean(),\n",
        "        'normal_error_std': normal_errors.std(),\n",
        "        'fraud_error_mean': results_individual[results_individual['true_class'] == 1]['recon_error'].mean(),\n",
        "        'fraud_error_std': results_individual[results_individual['true_class'] == 1]['recon_error'].std()\n",
        "    }\n",
        "\n",
        "}, model_filename)\n",
        "\n",
        "print(f\" Enhanced Transformer + VAE model saved successfully!\")\n",
        "print(f\" Location: {model_filename}\")\n",
        "print(f\" Performance: ROC AUC = {enhanced_roc_auc:.4f}\")\n",
        "print(f\" Precision = {precision:.4f}, Recall = {recall:.4f}\")\n",
        "print(f\" File size: {os.path.getsize(model_filename) / (1024*1024):.2f} MB\")\n",
        "print(f\"\\n Enhanced model saved to Google Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDflm-B76jG0",
        "outputId": "9a0a584d-e2e3-4a1d-dd51-839270b6e66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Saving Enhanced Transformer + VAE Model...\n",
            " Enhanced Transformer + VAE model saved successfully!\n",
            " Location: /content/drive/MyDrive/fraud_detection_models/enhanced_transformer_vae_roc_0.7680_20250726_1326.pth\n",
            " Performance: ROC AUC = 0.7680\n",
            " Precision = 0.0151, Recall = 0.2753\n",
            " File size: 13.63 MB\n",
            "\n",
            " Enhanced model saved to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import drive  #  This import was missing!\n",
        "# ================================\n",
        "# DEFINE MODEL-SPECIFIC VAE CLASSES\n",
        "# ================================\n",
        "\n",
        "class BaselineVAE(nn.Module):\n",
        "    \"\"\"Original VAE architecture for baseline model\"\"\"\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(BaselineVAE, self).__init__()\n",
        "\n",
        "        # Encoder (smaller architecture)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Note: Different layer names!\n",
        "        self.mu = nn.Linear(16, latent_dim)\n",
        "        self.logvar = nn.Linear(16, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, input_dim)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.mu(h)\n",
        "        logvar = self.logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "class EnhancedVAE(nn.Module):\n",
        "    \"\"\"Enhanced VAE architecture for transformer model\"\"\"\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(EnhancedVAE, self).__init__()\n",
        "\n",
        "        # Encoder (larger architecture)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Note: Different layer names!\n",
        "        self.mu_layer = nn.Linear(64, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(64, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "# ================================\n",
        "# CORRECTED LOADING FUNCTION\n",
        "# ================================\n",
        "\n",
        "def load_fraud_detection_models():\n",
        "    \"\"\"Load both baseline VAE and enhanced Transformer+VAE models\"\"\"\n",
        "\n",
        "    print(\" Loading fraud detection models from Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    models_dir = '/content/drive/MyDrive/fraud_detection_models'\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # ===== LOAD BASELINE VAE =====\n",
        "    baseline_files = glob.glob(f'{models_dir}/baseline_vae_*.pth')\n",
        "    if baseline_files:\n",
        "        best_baseline = max(baseline_files, key=lambda x: float(x.split('_roc_')[1].split('_')[0]))\n",
        "        print(f\" Loading baseline VAE from: {best_baseline}\")\n",
        "\n",
        "        baseline_checkpoint = torch.load(best_baseline, map_location='cpu', weights_only=False)\n",
        "\n",
        "        # Use BaselineVAE class (correct architecture)\n",
        "        baseline_vae = BaselineVAE(\n",
        "            input_dim=baseline_checkpoint['model_config']['input_dim'],\n",
        "            latent_dim=baseline_checkpoint['model_config']['latent_dim']\n",
        "        )\n",
        "        baseline_vae.load_state_dict(baseline_checkpoint['model_state_dict'])\n",
        "        baseline_vae.eval()\n",
        "\n",
        "        models['baseline'] = {\n",
        "            'model': baseline_vae,\n",
        "            'scaler': baseline_checkpoint['scaler'],\n",
        "            'threshold': baseline_checkpoint['training_info']['threshold'],\n",
        "            'roc_auc': baseline_checkpoint['training_info']['roc_auc'],\n",
        "            'results': baseline_checkpoint['results_baseline'],\n",
        "            'info': baseline_checkpoint['training_info']\n",
        "        }\n",
        "        print(f\" Baseline VAE loaded: ROC AUC = {models['baseline']['roc_auc']:.4f}\")\n",
        "    else:\n",
        "        print(\"  No baseline VAE found\")\n",
        "        models['baseline'] = None\n",
        "\n",
        "    # ===== LOAD ENHANCED TRANSFORMER + VAE =====\n",
        "    enhanced_files = glob.glob(f'{models_dir}/enhanced_transformer_vae_*.pth')\n",
        "    if enhanced_files:\n",
        "        best_enhanced = max(enhanced_files, key=lambda x: float(x.split('_roc_')[1].split('_')[0]))\n",
        "        print(f\" Loading enhanced Transformer+VAE from: {best_enhanced}\")\n",
        "\n",
        "        enhanced_checkpoint = torch.load(best_enhanced, map_location='cpu', weights_only=False)\n",
        "\n",
        "        # Recreate transformer\n",
        "        enhanced_transformer = TransactionTransformer(\n",
        "            feature_dim=enhanced_checkpoint['model_config']['transformer_config']['feature_dim'],\n",
        "            d_model=enhanced_checkpoint['model_config']['transformer_config']['d_model'],\n",
        "            nhead=enhanced_checkpoint['model_config']['transformer_config']['nhead'],\n",
        "            num_layers=enhanced_checkpoint['model_config']['transformer_config']['num_layers']\n",
        "        )\n",
        "        enhanced_transformer.load_state_dict(enhanced_checkpoint['transformer_state_dict'])\n",
        "        enhanced_transformer.eval()\n",
        "\n",
        "        # Use EnhancedVAE class (correct architecture)\n",
        "        enhanced_vae = EnhancedVAE(\n",
        "            input_dim=enhanced_checkpoint['model_config']['vae_config']['input_dim'],\n",
        "            latent_dim=enhanced_checkpoint['model_config']['vae_config']['latent_dim']\n",
        "        )\n",
        "        enhanced_vae.load_state_dict(enhanced_checkpoint['vae_state_dict'])\n",
        "        enhanced_vae.eval()\n",
        "\n",
        "        models['enhanced'] = {\n",
        "            'transformer': enhanced_transformer,\n",
        "            'vae': enhanced_vae,\n",
        "            'scaler': enhanced_checkpoint['scaler'],\n",
        "            'threshold': enhanced_checkpoint['training_info']['threshold_value'],\n",
        "            'roc_auc': enhanced_checkpoint['training_info']['roc_auc'],\n",
        "            'results': enhanced_checkpoint['results_enhanced'],\n",
        "            'info': enhanced_checkpoint['training_info']\n",
        "        }\n",
        "        print(f\" Enhanced Transformer+VAE loaded: ROC AUC = {models['enhanced']['roc_auc']:.4f}\")\n",
        "    else:\n",
        "        print(\"  No enhanced Transformer+VAE found\")\n",
        "        models['enhanced'] = None\n",
        "\n",
        "    return models\n",
        "\n",
        "# Load both models\n",
        "models = load_fraud_detection_models()\n",
        "\n",
        "# Set up variables for easy access\n",
        "if models['baseline'] is not None:\n",
        "    vae_baseline = models['baseline']['model']\n",
        "    scaler_baseline = models['baseline']['scaler']\n",
        "    threshold_baseline = models['baseline']['threshold']\n",
        "    baseline_roc_auc_score = models['baseline']['roc_auc']\n",
        "\n",
        "if models['enhanced'] is not None:\n",
        "    transformer_enhanced = models['enhanced']['transformer']\n",
        "    vae_enhanced = models['enhanced']['vae']\n",
        "    scaler_enhanced = models['enhanced']['scaler']\n",
        "    threshold_enhanced = models['enhanced']['threshold']\n",
        "    enhanced_roc_auc_score = models['enhanced']['roc_auc']\n",
        "\n",
        "# Display comparison\n",
        "print(\"\\n MODEL COMPARISON SUMMARY:\")\n",
        "if models['baseline'] is not None and models['enhanced'] is not None:\n",
        "    baseline_auc = models['baseline']['roc_auc']\n",
        "    enhanced_auc = models['enhanced']['roc_auc']\n",
        "    improvement = enhanced_auc - baseline_auc\n",
        "    improvement_pct = (improvement / baseline_auc) * 100\n",
        "\n",
        "    print(f\" Baseline VAE:           ROC AUC = {baseline_auc:.4f}\")\n",
        "    print(f\" Enhanced Transformer:   ROC AUC = {enhanced_auc:.4f}\")\n",
        "    print(f\" Improvement:            {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzfNM5ez7LxO",
        "outputId": "4a1b0f30-a33f-4bde-91bf-f5e04ebf6934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading fraud detection models from Google Drive...\n",
            "Mounted at /content/drive\n",
            " Loading baseline VAE from: /content/drive/MyDrive/fraud_detection_models/baseline_vae_roc_0.5989_20250726_1229.pth\n",
            " Baseline VAE loaded: ROC AUC = 0.5989\n",
            " Loading enhanced Transformer+VAE from: /content/drive/MyDrive/fraud_detection_models/enhanced_transformer_vae_roc_0.7680_20250726_1326.pth\n",
            " Enhanced Transformer+VAE loaded: ROC AUC = 0.7680\n",
            "\n",
            " MODEL COMPARISON SUMMARY:\n",
            " Baseline VAE:           ROC AUC = 0.5989\n",
            " Enhanced Transformer:   ROC AUC = 0.7680\n",
            " Improvement:            +0.1692 (+28.2%)\n"
          ]
        }
      ]
    }
  ]
}